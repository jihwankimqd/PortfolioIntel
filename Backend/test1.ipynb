{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600494649519",
   "display_name": "Python 3.7.3 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "import json\n",
    "import pymongo\n",
    "from flask import Flask, jsonify, request\n",
    "from flask_cors import CORS, cross_origin\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# configuration\n",
    "DEBUG = True\n",
    "\n",
    "# instantiate the app\n",
    "app = Flask(__name__)\n",
    "# app.config.from_object(__name__)\n",
    "app.config['CORS_HEADERS'] = 'Content-Type'\n",
    "# enable CORS\n",
    "CORS(app, resources={r'/*': {'origins': '*'}})\n",
    "\n",
    "# app = Flask(__name__)\n",
    "# cors = CORS(app)\n",
    "# app.config['CORS_HEADERS'] = 'Content-Type'\n",
    "\n",
    "cluster = MongoClient('mongodb+srv://jihwan:1234@intelportfolio.kqupw.gcp.mongodb.net/test?retryWrites=true&w=majority')\n",
    "db = cluster['test']\n",
    "\n",
    "## BACKEND and ML\n",
    "\n",
    "# DJI\n",
    "collection_DJI = db['DJI'].find({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_DJI = pd.DataFrame(list(collection_DJI))\n",
    "del df_DJI['_id']\n",
    "\n",
    "df_DJI.info()\n",
    "df_DJI.set_index(['Date'],inplace=True)\n",
    "df_DJI.sort_values(by=['Date'],ascending=False,inplace=True)\n",
    "drop_cols = ['Open', 'High','Low', 'Adj Close', 'Volume']\n",
    "\n",
    "# Remove columns without relative significance.\n",
    "df_DJI = df_DJI.drop(drop_cols,axis=1)\n",
    "df_DJI.head()\n",
    "\n",
    "\n",
    "df_DJI.index = pd.to_datetime(df_DJI.index)\n",
    "\n",
    "df_DJI = df_DJI.resample('D').asfreq()\n",
    "df_DJI.sort_values(by=['Date'],ascending=False,inplace=True)\n",
    "df_DJI.fillna(method='ffill',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DJI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_DJI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KR_IR import get_IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IR = get_IR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WTI import get_WTI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_WTI = get_WTI()\n",
    "df_WTI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from XR import get_XR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_XR = get_XR()\n",
    "df_XR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "\n",
    "## Data collection - Scraping stock data\n",
    "\n",
    "\n",
    "# This cell sets up the code to scrape the stock data from Naver Finance for SK Innovation(096770). The reason that SK Innovation was chosen was because I made over 50% of my initial amount by trading this stock over a short period, and I wanted to apply my ad-hoc logic to a systematic and reproducible method, hence this project.\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Change the stock ticker to collect different stock data\n",
    "def preprocessdata(stock_input):\n",
    "    # stock_id = '096770'\n",
    "    stock_id = str(stock_input)\n",
    "\n",
    "    def get_stocks(stock_id,max_limit):\n",
    "        cate = []\n",
    "        stocks = [] \n",
    "\n",
    "        count = 1\n",
    "\n",
    "        while count < max_limit:\n",
    "            base_url = 'https://finance.naver.com/item/frgn.nhn?code='+str(stock_id)+'&page='+str(count)\n",
    "            res = requests.get(base_url)\n",
    "            html = BeautifulSoup(res.content,'html.parser')\n",
    "            # Multiple tables of same name and class. Therefore use findAll and select the wanted table.\n",
    "            table_all = html.findAll('table',{'class':'type2'})\n",
    "            table_0 = table_all[1]\n",
    "            cate_0 = table_0.find_all('th')\n",
    "            main_0 = table_0.find_all('span')\n",
    "            # span_0 = main_0.find('span')\n",
    "\n",
    "            # For first iteration (count==1) append the headers to cate.\n",
    "            if count == 1:\n",
    "                for i in range(len(cate_0)):\n",
    "                    if (i == 5 or i== 6):\n",
    "                        continue\n",
    "                    else:\n",
    "                        cate.append(cate_0[i].text)\n",
    "\n",
    "                date = []\n",
    "                close = []\n",
    "                change = []\n",
    "                percentage_change = []\n",
    "                volume = []\n",
    "                org_volume = []\n",
    "                foreign_volume = []\n",
    "                foreign_count = []\n",
    "                foreign_percentage = []\n",
    "\n",
    "            for i in range(len(main_0)):\n",
    "\n",
    "                if(i%9 == 0):\n",
    "                    date.append(main_0[i].text)\n",
    "                if(i%9 == 1):\n",
    "                    close.append(main_0[i].text)\n",
    "                if(i%9 == 2):\n",
    "                    change.append(main_0[i].text)\n",
    "                if(i%9 == 3):\n",
    "                    percentage_change.append(main_0[i].text)\n",
    "                if(i%9 == 4):\n",
    "                    volume.append(main_0[i].text)\n",
    "                if(i%9 == 5):\n",
    "                    org_volume.append(main_0[i].text)\n",
    "                if(i%9 == 6):\n",
    "                    foreign_volume.append(main_0[i].text)\n",
    "                if(i%9 == 7):\n",
    "                    foreign_count.append(main_0[i].text)\n",
    "                if(i%9 == 8):\n",
    "                    foreign_percentage.append(main_0[i].text)\n",
    "            \n",
    "            df_data = [date, close, change, percentage_change, volume, org_volume, foreign_volume, foreign_count, foreign_percentage]       \n",
    "            # print(cate)\n",
    "\n",
    "            count += 1\n",
    "        df = pd.DataFrame(columns = cate)\n",
    "        df_data = ['Date', 'Close', 'Change', 'Pct_Change', 'Volume', 'Org_Volume', 'Foreign_Volume', 'Foreign_Count', 'Foreign_Pct'] \n",
    "        df.columns = df_data\n",
    "        # df['날짜'] = date\n",
    "        # df['종가'] = close\n",
    "        # df['전일비'] = change\n",
    "        # df['등락률'] = percentage_change\n",
    "        # df['거래량'] = volume\n",
    "        # df['순매매량'] = org_volume\n",
    "        # df['순매매량'] = foreign_volume\n",
    "        # df['보유주수'] = foreign_count\n",
    "        # df['보유율'] = foreign_percentage\n",
    "        df['Date'] = date\n",
    "        df['Close'] = close\n",
    "        df['Change'] = change\n",
    "        df['Pct_Change'] = percentage_change\n",
    "        df['Volume'] = volume\n",
    "        df['Org_Volume'] = org_volume\n",
    "        df['Foreign_Volume'] = foreign_volume\n",
    "        df['Foreign_Count'] = foreign_count\n",
    "        df['Foreign_Pct'] = foreign_percentage\n",
    "    \n",
    "        # df['전일비'] = df['전일비'].map(lambda x: x.lstrip('\\n\\t').rstrip('\\n\\t'))\n",
    "        df['Change'] = df['Change'].map(lambda x: x.lstrip('\\n\\t').rstrip('\\n\\t'))\n",
    "        # df['등락률'] = df['등락률'].map(lambda x: x.lstrip('\\n\\t').rstrip('\\n\\t'))\n",
    "        df['Pct_Change'] = df['Pct_Change'].map(lambda x: x.lstrip('\\n\\t').rstrip('\\n\\t'))\n",
    "\n",
    "        \n",
    "        return df \n",
    "\n",
    "\n",
    "    ## Data Cleaning\n",
    "\n",
    "    # This cell runs the scraper funciton above and scrape the data, and may take a (few) minute(s).\n",
    "\n",
    "    df = get_stocks(stock_id,71)\n",
    "\n",
    "    # Remove special characters in dataframe\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.replace('\\,','', regex=True)\n",
    "    df = df.replace('\\+','', regex=True)\n",
    "    # df = df.replace('\\-','', regex=True)\n",
    "    df = df.replace('\\%','', regex=True)\n",
    "\n",
    "    # Set index as Date\n",
    "    df.set_index(df['Date'], inplace= True)\n",
    "    df.drop('Date',axis=1,inplace=True)\n",
    "\n",
    "    # df.head()\n",
    "\n",
    "    # Fill missing values such as weekends/holidays\n",
    "    df = df.resample('D').asfreq()\n",
    "    df.sort_values(by=['Date'],ascending=False,inplace=True)\n",
    "    df.fillna(method='ffill',inplace=True)\n",
    "\n",
    "\n",
    "    # stock_file = 'stock_'+ str(stock_id) +'.csv'\n",
    "    # df.to_csv(str(stock_file))\n",
    "\n",
    "\n",
    "    ## Tweak the KR_IR file\n",
    "    from KR_IR import get_IR\n",
    "    df_IR = get_IR()\n",
    "\n",
    "    ## Clean DJI.csv\n",
    "    from DJI import get_DJI\n",
    "    df_DJI = get_DJI()\n",
    "\n",
    "\n",
    "    ## Clean OIL_WTI.csv\n",
    "    from WTI import get_WTI\n",
    "    df_WTI = get_WTI()\n",
    "\n",
    "\n",
    "    ## Clean USD_KRW_XR.csv \n",
    "    from XR import get_XR\n",
    "    df_XR = get_XR()\n",
    "\n",
    "    # create clone of df\n",
    "    df_STK = df\n",
    "\n",
    "    # All cleaned dataframes. But all dataframes have different shapes. Therefore, must unite into a single dataframe and order it by date.\n",
    "    # df_DJI.shape\n",
    "    # df_IR.shape\n",
    "    # df_WTI.shape\n",
    "    # df_XR.shape\n",
    "    # df_STK.shape\n",
    "\n",
    "    # df_STK.tail()\n",
    "    start_date = '2015-08-10'\n",
    "    end_date = '2020-08-10'\n",
    "\n",
    "    df_DJI = df_DJI[(df_DJI.index >= start_date) & (df_DJI.index <= end_date)]\n",
    "    df_IR = df_IR[(df_IR.index >= start_date) & (df_IR.index <= end_date)]\n",
    "    df_WTI = df_WTI[(df_WTI.index >= start_date) & (df_WTI.index <= end_date)]\n",
    "    df_XR = df_XR[(df_XR.index >= start_date) & (df_XR.index <= end_date)]\n",
    "    df_STK = df_STK[(df_STK.index >= start_date) & (df_STK.index <= end_date)]\n",
    "\n",
    "    # print('Shape of DJI: ', df_DJI.shape)\n",
    "    # print('Shape of IR: ', df_IR.shape)\n",
    "    # print('Shape of WTI: ', df_WTI.shape)\n",
    "    # print('Shape of XR: ', df_XR.shape)\n",
    "    # print('Shape of STK: ', df_STK.shape)\n",
    "\n",
    "\n",
    "    # Concat all dataframes into one\n",
    "\n",
    "    df_DJI.columns = ['DJI_Close']\n",
    "    df_XR.columns = ['XR','XR_Pct_Change']\n",
    "    df = pd.concat([df_STK, df_DJI,df_IR,df_WTI,df_XR], axis=1)\n",
    "\n",
    "    # df.head()\n",
    "    # df.tail()\n",
    "\n",
    "    df.to_csv('processed_data.csv')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocessdata('096770')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pymongo\n",
    "from flask import Flask, jsonify, request\n",
    "from flask_cors import CORS, cross_origin\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# configuration\n",
    "DEBUG = True\n",
    "\n",
    "# instantiate the app\n",
    "app = Flask(__name__)\n",
    "# app.config.from_object(__name__)\n",
    "app.config['CORS_HEADERS'] = 'Content-Type'\n",
    "# enable CORS\n",
    "CORS(app, resources={r'/*': {'origins': '*'}})\n",
    "\n",
    "# app = Flask(__name__)\n",
    "# cors = CORS(app)\n",
    "# app.config['CORS_HEADERS'] = 'Content-Type'\n",
    "\n",
    "cluster = MongoClient('mongodb+srv://jihwan:1234@intelportfolio.kqupw.gcp.mongodb.net/test?retryWrites=true&w=majority')\n",
    "db = cluster['test']\n",
    "collection_preprocessed_data = db['preprocessed_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    data = pd.read_csv('processed_data.csv')\n",
    "    data_json = json.loads(data.to_json(orient='records'))\n",
    "    collection_preprocessed_data .remove()\n",
    "    collection_preprocessed_data .insert(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    collection_preprocessed_data .remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    stock_input = '096770'\n",
    "    preprocessdata(stock_input)\n",
    "    data = pd.read_csv('processed_data.csv')\n",
    "    data_json = json.loads(data.to_json(orient='records'))\n",
    "    collection_preprocessed_data .remove()\n",
    "    collection_preprocessed_data .insert(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-840d43c7e61b>, line 27)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-840d43c7e61b>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    from './DataAnalysis/datapreprocessing' import preprocessdata\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pymongo\n",
    "from flask import Flask, jsonify, request\n",
    "from flask_cors import CORS, cross_origin\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# configuration\n",
    "DEBUG = True\n",
    "\n",
    "# instantiate the app\n",
    "app = Flask(__name__)\n",
    "# app.config.from_object(__name__)\n",
    "app.config['CORS_HEADERS'] = 'Content-Type'\n",
    "# enable CORS\n",
    "CORS(app, resources={r'/*': {'origins': '*'}})\n",
    "\n",
    "# app = Flask(__name__)\n",
    "# cors = CORS(app)\n",
    "# app.config['CORS_HEADERS'] = 'Content-Type'\n",
    "\n",
    "cluster = MongoClient('mongodb+srv://jihwan:1234@intelportfolio.kqupw.gcp.mongodb.net/test?retryWrites=true&w=majority')\n",
    "db = cluster['test']\n",
    "\n",
    "## BACKEND and ML\n",
    "collection_preprocessed_data = db['preprocessed_data']\n",
    "\n",
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '~/Desktop/portfolio_intel/Backend/DataAnalysis')\n",
    "\n",
    "from datapreprocessing import preprocessdata\n",
    "\n",
    "stock_input = '096770'\n",
    "preprocessdata(stock_input)\n",
    "data = pd.read_csv('processed_data.csv')\n",
    "data_json = json.loads(data.to_json(orient='records'))\n",
    "collection_preprocessed_data.remove()\n",
    "collection_preprocessed_data.insert(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}