{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600492884797",
   "display_name": "Python 3.7.3 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "import json\n",
    "import pymongo\n",
    "from flask import Flask, jsonify, request\n",
    "from flask_cors import CORS, cross_origin\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# configuration\n",
    "DEBUG = True\n",
    "\n",
    "# instantiate the app\n",
    "app = Flask(__name__)\n",
    "# app.config.from_object(__name__)\n",
    "app.config['CORS_HEADERS'] = 'Content-Type'\n",
    "# enable CORS\n",
    "CORS(app, resources={r'/*': {'origins': '*'}})\n",
    "\n",
    "# app = Flask(__name__)\n",
    "# cors = CORS(app)\n",
    "# app.config['CORS_HEADERS'] = 'Content-Type'\n",
    "\n",
    "cluster = MongoClient('mongodb+srv://jihwan:1234@intelportfolio.kqupw.gcp.mongodb.net/test?retryWrites=true&w=majority')\n",
    "db = cluster['test']\n",
    "\n",
    "## BACKEND and ML\n",
    "\n",
    "# DJI\n",
    "collection_DJI = db['DJI'].find({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_DJI = pd.DataFrame(list(collection_DJI))\n",
    "del df_DJI['_id']\n",
    "\n",
    "df_DJI.info()\n",
    "df_DJI.set_index(['Date'],inplace=True)\n",
    "df_DJI.sort_values(by=['Date'],ascending=False,inplace=True)\n",
    "drop_cols = ['Open', 'High','Low', 'Adj Close', 'Volume']\n",
    "\n",
    "# Remove columns without relative significance.\n",
    "df_DJI = df_DJI.drop(drop_cols,axis=1)\n",
    "df_DJI.head()\n",
    "\n",
    "\n",
    "df_DJI.index = pd.to_datetime(df_DJI.index)\n",
    "\n",
    "df_DJI = df_DJI.resample('D').asfreq()\n",
    "df_DJI.sort_values(by=['Date'],ascending=False,inplace=True)\n",
    "df_DJI.fillna(method='ffill',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DJI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DJI import get_DJI\n",
    "get_DJI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_DJI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KR_IR import get_IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='Date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datapreprocessing import preprocessdata\n",
    "preprocessdata('096770')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('processed_data.csv')\n",
    "df = df.sort_values(by='Date')\n",
    "df.head()\n",
    "# df = df.reset_index(drop = True)\n",
    "# df.to_csv('processed_data1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='Date')\n",
    "# df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.reset_index(drop=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "from datetime import datetime as dt\n",
    "\n",
    "stock_id = '005930'\n",
    "dataframe = web.DataReader(stock_id, 'naver', start='2015-01-01', end=dt.now()).reset_index()\n",
    "dataframe['Date'] = pd.to_datetime(dataframe['Date']).dt.date\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader.data as web\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "stock_id = '005930'\n",
    "# Used an API, because the above scraping method took too long.\n",
    "df = web.DataReader(stock_id, 'naver', start='2015-01-01', end=dt.now()).reset_index()\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Fill missing values such as weekends/holidays\n",
    "df = df.resample('D').asfreq()\n",
    "df.fillna(method='ffill',inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "\n",
    "## Data collection - Scraping stock data\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas_datareader.data as web\n",
    "from datetime import datetime as dt\n",
    "\n",
    "\n",
    "# Change the stock ticker to collect different stock data\n",
    "def preprocessdata(stock_input):\n",
    "    stock_id = str(stock_input)\n",
    "  \n",
    "    stock_id = '005930'\n",
    "    # Used an API, because the above scraping method took too long.\n",
    "    df = web.DataReader(stock_id, 'naver', start='2015-01-01', end=dt.now()).reset_index()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # Fill missing values such as weekends/holidays\n",
    "    df = df.resample('D').asfreq()\n",
    "    df.fillna(method='ffill',inplace=True)\n",
    "    df.head()\n",
    "\n",
    "    ## Tweak the KR_IR file\n",
    "    from KR_IR import get_IR\n",
    "    df_IR = get_IR()\n",
    "\n",
    "    ## Clean DJI.csv\n",
    "    from DJI import get_DJI\n",
    "    df_DJI = get_DJI()\n",
    "\n",
    "\n",
    "    ## Clean OIL_WTI.csv\n",
    "    from WTI import get_WTI\n",
    "    df_WTI = get_WTI()\n",
    "\n",
    "\n",
    "    ## Clean USD_KRW_XR.csv \n",
    "    from XR import get_XR\n",
    "    df_XR = get_XR()\n",
    "\n",
    "    # create clone of df\n",
    "    df_STK = df\n",
    "\n",
    "    # All cleaned dataframes. But all dataframes have different shapes. Therefore, must unite into a single dataframe and order it by date.\n",
    "    # df_DJI.shape\n",
    "    # df_IR.shape\n",
    "    # df_WTI.shape\n",
    "    # df_XR.shape\n",
    "    # df_STK.shape\n",
    "\n",
    "    # df_STK.tail()\n",
    "    start_date = '2015-08-10'\n",
    "    end_date = '2020-08-10'\n",
    "\n",
    "    df_DJI = df_DJI[(df_DJI.index >= start_date) & (df_DJI.index <= end_date)]\n",
    "    df_IR = df_IR[(df_IR.index >= start_date) & (df_IR.index <= end_date)]\n",
    "    df_WTI = df_WTI[(df_WTI.index >= start_date) & (df_WTI.index <= end_date)]\n",
    "    df_XR = df_XR[(df_XR.index >= start_date) & (df_XR.index <= end_date)]\n",
    "    df_STK = df_STK[(df_STK.index >= start_date) & (df_STK.index <= end_date)]\n",
    "\n",
    "    # print('Shape of DJI: ', df_DJI.shape)\n",
    "    # print('Shape of IR: ', df_IR.shape)\n",
    "    # print('Shape of WTI: ', df_WTI.shape)\n",
    "    # print('Shape of XR: ', df_XR.shape)\n",
    "    # print('Shape of STK: ', df_STK.shape)\n",
    "\n",
    "\n",
    "    # Concat all dataframes into one\n",
    "\n",
    "    df_DJI.columns = ['DJI_Close']\n",
    "    df_XR.columns = ['XR','XR_Pct_Change']\n",
    "    df = pd.concat([df_STK, df_DJI,df_IR,df_WTI,df_XR], axis=1)\n",
    "\n",
    "    # df.head()\n",
    "    # df.tail()\n",
    "\n",
    "\n",
    "    df.to_csv('processed_data.csv')\n",
    "\n",
    "    # df = pd.read_csv('processed_data.csv')\n",
    "    # df = df.sort_values(by='Date')\n",
    "    # df = df.reset_index(drop = True)\n",
    "    return\n",
    "\n",
    "preprocessdata('005930')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "\n",
    "## Data collection - Scraping stock data\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas_datareader.data as web\n",
    "from datetime import datetime as dt\n",
    "\n",
    "\n",
    "# Change the stock ticker to collect different stock data\n",
    "def preprocessdata(stock_input):\n",
    "    # stock_id = '096770'\n",
    "    stock_id = str(stock_input)\n",
    "    # def get_stocks(stock_id,max_limit):\n",
    "    #     cate = []\n",
    "    #     stocks = [] \n",
    "\n",
    "    #     count = 1\n",
    "\n",
    "    #     while count < max_limit:\n",
    "    #         base_url = 'https://finance.naver.com/item/frgn.nhn?code='+str(stock_id)+'&page='+str(count)\n",
    "    #         res = requests.get(base_url)\n",
    "    #         html = BeautifulSoup(res.content,'html.parser')\n",
    "    #         # Multiple tables of same name and class. Therefore use findAll and select the wanted table.\n",
    "    #         table_all = html.findAll('table',{'class':'type2'})\n",
    "    #         table_0 = table_all[1]\n",
    "    #         cate_0 = table_0.find_all('th')\n",
    "    #         main_0 = table_0.find_all('span')\n",
    "    #         # span_0 = main_0.find('span')\n",
    "\n",
    "    #         # For first iteration (count==1) append the headers to cate.\n",
    "    #         if count == 1:\n",
    "    #             for i in range(len(cate_0)):\n",
    "    #                 if (i == 5 or i== 6):\n",
    "    #                     continue\n",
    "    #                 else:\n",
    "    #                     cate.append(cate_0[i].text)\n",
    "\n",
    "    #             date = []\n",
    "    #             close = []\n",
    "    #             change = []\n",
    "    #             percentage_change = []\n",
    "    #             volume = []\n",
    "    #             org_volume = []\n",
    "    #             foreign_volume = []\n",
    "    #             foreign_count = []\n",
    "    #             foreign_percentage = []\n",
    "\n",
    "    #         for i in range(len(main_0)):\n",
    "\n",
    "    #             if(i%9 == 0):\n",
    "    #                 date.append(main_0[i].text)\n",
    "    #             if(i%9 == 1):\n",
    "    #                 close.append(main_0[i].text)\n",
    "    #             if(i%9 == 2):\n",
    "    #                 change.append(main_0[i].text)\n",
    "    #             if(i%9 == 3):\n",
    "    #                 percentage_change.append(main_0[i].text)\n",
    "    #             if(i%9 == 4):\n",
    "    #                 volume.append(main_0[i].text)\n",
    "    #             if(i%9 == 5):\n",
    "    #                 org_volume.append(main_0[i].text)\n",
    "    #             if(i%9 == 6):\n",
    "    #                 foreign_volume.append(main_0[i].text)\n",
    "    #             if(i%9 == 7):\n",
    "    #                 foreign_count.append(main_0[i].text)\n",
    "    #             if(i%9 == 8):\n",
    "    #                 foreign_percentage.append(main_0[i].text)\n",
    "            \n",
    "    #         df_data = [date, close, change, percentage_change, volume, org_volume, foreign_volume, foreign_count, foreign_percentage]       \n",
    "    #         # print(cate)\n",
    "\n",
    "    #         count += 1\n",
    "    #     df = pd.DataFrame(columns = cate)\n",
    "    #     df_data = ['Date', 'Close', 'Change', 'Pct_Change', 'Volume', 'Org_Volume', 'Foreign_Volume', 'Foreign_Count', 'Foreign_Pct'] \n",
    "    #     df.columns = df_data\n",
    "    #     # df['날짜'] = date\n",
    "    #     # df['종가'] = close\n",
    "    #     # df['전일비'] = change\n",
    "    #     # df['등락률'] = percentage_change\n",
    "    #     # df['거래량'] = volume\n",
    "    #     # df['순매매량'] = org_volume\n",
    "    #     # df['순매매량'] = foreign_volume\n",
    "    #     # df['보유주수'] = foreign_count\n",
    "    #     # df['보유율'] = foreign_percentage\n",
    "    #     df['Date'] = date\n",
    "    #     df['Close'] = close\n",
    "    #     df['Change'] = change\n",
    "    #     df['Pct_Change'] = percentage_change\n",
    "    #     df['Volume'] = volume\n",
    "    #     df['Org_Volume'] = org_volume\n",
    "    #     df['Foreign_Volume'] = foreign_volume\n",
    "    #     df['Foreign_Count'] = foreign_count\n",
    "    #     df['Foreign_Pct'] = foreign_percentage\n",
    "    \n",
    "    #     # df['전일비'] = df['전일비'].map(lambda x: x.lstrip('\\n\\t').rstrip('\\n\\t'))\n",
    "    #     df['Change'] = df['Change'].map(lambda x: x.lstrip('\\n\\t').rstrip('\\n\\t'))\n",
    "    #     # df['등락률'] = df['등락률'].map(lambda x: x.lstrip('\\n\\t').rstrip('\\n\\t'))\n",
    "    #     df['Pct_Change'] = df['Pct_Change'].map(lambda x: x.lstrip('\\n\\t').rstrip('\\n\\t'))\n",
    "\n",
    "        \n",
    "    #     return df \n",
    "\n",
    "\n",
    "    ## Data Cleaning\n",
    "\n",
    "    # This cell runs the scraper funciton above and scrape the data, and may take a (few) minute(s).\n",
    "\n",
    "    # df = get_stocks(stock_id,71)\n",
    "\n",
    "    # # Remove special characters in dataframe\n",
    "    # df['Date'] = pd.to_datetime(df['Date'])\n",
    "    # df = df.replace('\\,','', regex=True)\n",
    "    # df = df.replace('\\+','', regex=True)\n",
    "    # # df = df.replace('\\-','', regex=True)\n",
    "    # df = df.replace('\\%','', regex=True)\n",
    "\n",
    "    # # Set index as Date\n",
    "    # df.set_index(df['Date'], inplace= True)\n",
    "    # df.drop('Date',axis=1,inplace=True)\n",
    "\n",
    "    # # df.head()\n",
    "\n",
    "    # # Fill missing values such as weekends/holidays\n",
    "    # df = df.resample('D').asfreq()\n",
    "    # df.sort_values(by=['Date'],ascending=False,inplace=True)\n",
    "    # df.fillna(method='ffill',inplace=True)\n",
    "\n",
    "    # stock_file = 'stock_'+ str(stock_id) +'.csv'\n",
    "    # df.to_csv(str(stock_file))\n",
    "\n",
    "    # Used an API, because the above scraping method took too long.\n",
    "    df = web.DataReader(stock_id, 'naver', start='2015-01-01', end=dt.now()).reset_index()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # Fill missing values such as weekends/holidays\n",
    "    df = df.resample('D').asfreq()\n",
    "    df.fillna(method='ffill',inplace=True)\n",
    "    # df.head()\n",
    "\n",
    "    ## Tweak the KR_IR file\n",
    "    from KR_IR import get_IR\n",
    "    df_IR = get_IR()\n",
    "\n",
    "    ## Clean DJI.csv\n",
    "    from DJI import get_DJI\n",
    "    df_DJI = get_DJI()\n",
    "\n",
    "\n",
    "    ## Clean OIL_WTI.csv\n",
    "    from WTI import get_WTI\n",
    "    df_WTI = get_WTI()\n",
    "\n",
    "\n",
    "    ## Clean USD_KRW_XR.csv \n",
    "    from XR import get_XR\n",
    "    df_XR = get_XR()\n",
    "\n",
    "    # create clone of df\n",
    "    df_STK = df\n",
    "\n",
    "    # All cleaned dataframes. But all dataframes have different shapes. Therefore, must unite into a single dataframe and order it by date.\n",
    "    # df_DJI.shape\n",
    "    # df_IR.shape\n",
    "    # df_WTI.shape\n",
    "    # df_XR.shape\n",
    "    # df_STK.shape\n",
    "\n",
    "    # df_STK.tail()\n",
    "    start_date = '2015-08-10'\n",
    "    end_date = '2020-08-10'\n",
    "\n",
    "    df_DJI = df_DJI[(df_DJI.index >= start_date) & (df_DJI.index <= end_date)]\n",
    "    df_IR = df_IR[(df_IR.index >= start_date) & (df_IR.index <= end_date)]\n",
    "    df_WTI = df_WTI[(df_WTI.index >= start_date) & (df_WTI.index <= end_date)]\n",
    "    df_XR = df_XR[(df_XR.index >= start_date) & (df_XR.index <= end_date)]\n",
    "    df_STK = df_STK[(df_STK.index >= start_date) & (df_STK.index <= end_date)]\n",
    "\n",
    "    # print('Shape of DJI: ', df_DJI.shape)\n",
    "    # print('Shape of IR: ', df_IR.shape)\n",
    "    # print('Shape of WTI: ', df_WTI.shape)\n",
    "    # print('Shape of XR: ', df_XR.shape)\n",
    "    # print('Shape of STK: ', df_STK.shape)\n",
    "\n",
    "\n",
    "    # Concat all dataframes into one\n",
    "\n",
    "    df_DJI.columns = ['DJI_Close']\n",
    "    df_XR.columns = ['XR','XR_Pct_Change']\n",
    "    df = pd.concat([df_STK, df_DJI,df_IR,df_WTI,df_XR], axis=1)\n",
    "\n",
    "    # df.head()\n",
    "    # df.tail()\n",
    "\n",
    "\n",
    "    df.to_csv('processed_data.csv')\n",
    "\n",
    "    # df = pd.read_csv('processed_data.csv')\n",
    "    # df = df.sort_values(by='Date')\n",
    "    # df = df.reset_index(drop = True)\n",
    "    return\n",
    "\n",
    "preprocessdata('005930')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "\n",
    "## EDA\n",
    "# Objective of this section is to find insights in the preprocessed data.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('processed_data.csv')\n",
    "df.head()\n",
    "\n",
    "\n",
    "# corr_matrix = df.corr()\n",
    "# corr_matrix\n",
    "\n",
    "# The label's (Close) correlation with other features is of interest. Features which show exceptionally low correlation to the label should be dropped. For example-> Change, Org_Volume, XR_Pct_Change have very low correlation and should be dropped.\n",
    "\n",
    "\n",
    "# from pandas.plotting import scatter_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "attributes = ['Close','Volume','Foreign_Count','DJI_Close','IR','XR']\n",
    "\n",
    "# Uncomment to view the plot. Plot takes up a lot of memory and time when viewed on Github and hence was commented out.\n",
    "\n",
    "# pd.plotting.scatter_matrix(df[attributes],figsize=(12,8),alpha=0.1)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "### The scatter plot reveals a lot about the relationship between the features and label. Amongst the plots, there seems to be a strong postivie linear correlation between Foreign_Count, DJI_Close and Close, whilst a strong negative correlation between XR and Close. For Volume, it is less obvious, but there exists a negative linear trend with Close. As for IR, there exists specific regions of IR values shown as a group points in a '|' shape, and there seems to be linearity between increasing IR and higher Close prices.\n",
    "\n",
    "\n",
    "drop_cols = ['XR_Pct_Change']\n",
    "df.drop(drop_cols,axis=1,inplace=True)\n",
    "# df.head()\n",
    "\n",
    "\n",
    "### There are no categorical variables to encode and transformation pipelines are not needed because the data was preprocessed in the data_preprocessing step. In hindsight, the usage of pipelines may provide cleaner code with very scalability, but at this point, for this specific project, using it would be redundant and a waste of resources.\n",
    "\n",
    "\n",
    "## Feature Scaling\n",
    "### Standarization vs Normalization?\n",
    "### Standarization is much less prone to outliers, but in this specific case, some variable changes such as closing price or volume may have sudden extreme changes following a series of events (such as an earnings report/significant news). Therefore, these extremes may be regarded as outliers in standaization, and this is not desired. Extremes must be included in the data to allow a better prediction of the stock price movement. Therefore, normalization should be used for feature scaling.\n",
    "\n",
    "# Copy the df for testing purposes\n",
    "df_test = df\n",
    "df_test = df_test.set_index(['Date'])\n",
    "df_test['USD_Bar'] = pd.to_numeric(df_test['USD_Bar'], errors='coerce')\n",
    "# df_test['USD_Bar'].dtype\n",
    "# df_test.info()\n",
    "# df_test.head()\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Min-Max Standardized df_norm\n",
    "x = df_test.values \n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df_norm = pd.DataFrame(x_scaled)\n",
    "# Date,Open,High,Low,Close,Volume,DJI_Close,IR,USD_Bar,XR,XR_Pct_Change\n",
    "df_norm.columns = ['Open', 'High', 'Low', 'Close', 'Volume',\n",
    "       'DJI_Close', 'IR', 'USD_Bar', 'XR']\n",
    "\n",
    "# df_norm.head()\n",
    "\n",
    "\n",
    "# Check for null values and fill using forward fill.\n",
    "\n",
    "df_norm.isnull().any()\n",
    "df_norm = df_norm.fillna(method='ffill')\n",
    "\n",
    "# same for df_test (non-normalized dataset)\n",
    "df_test = df_test.fillna(method='ffill')\n",
    "\n",
    "\n",
    "## Selecting and Training a model\n",
    "\n",
    "\n",
    "# Split the data into training and testing datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = df_test.drop(\"Close\", axis=1) # drop labels for training set\n",
    "labels = df_test[\"Close\"].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels,test_size=0.1 ,random_state=41)\n",
    "\n",
    "# X_train.shape\n",
    "\n",
    "\n",
    "# Linear Regression\n",
    "\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# lin_reg = LinearRegression()\n",
    "# lin_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# print(\"Predictions:\", lin_reg.predict(X_test))\n",
    "\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import numpy as np\n",
    "\n",
    "# stock_predictions = lin_reg.predict(X_test)\n",
    "# lin_mse = mean_squared_error(y_test, stock_predictions)\n",
    "# lin_rmse = np.sqrt(lin_mse)\n",
    "# lin_rmse\n",
    "\n",
    "\n",
    "# 14830 KRW is roughly in the 10% range of the stock price. It's not bad, but maybe other models can fit better.\n",
    "\n",
    "\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# lin_mae = mean_absolute_error(y_test, stock_predictions)\n",
    "# lin_mae\n",
    "\n",
    "\n",
    "# To choose which metric to test the error (although it depends on the loss function), RMSE or MAE were considered, but RMSE gives more weight to points further away from mean, and should be reference in this particular stock scenario.\n",
    "\n",
    "\n",
    "# Decision Tree Regressor\n",
    "\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# tree_reg = DecisionTreeRegressor(random_state=41)\n",
    "# tree_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# stock_predictions = tree_reg.predict(X_test)\n",
    "# tree_mse = mean_squared_error(y_test, stock_predictions)\n",
    "# tree_rmse = np.sqrt(tree_mse)\n",
    "# tree_rmse\n",
    "\n",
    "\n",
    "# The RMSE is much lower, which may either indicate overfitting or a better model.\n",
    "\n",
    "\n",
    "# Test using cross validation\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# scores = cross_val_score(tree_reg, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "# tree_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "\n",
    "# def display_scores(scores):\n",
    "#     print(\"Scores:\", scores)\n",
    "#     print(\"Mean:\", scores.mean())\n",
    "#     print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "# # Decision Tree RMSE Scores\n",
    "# display_scores(tree_rmse_scores)\n",
    "\n",
    "\n",
    "# lin_scores = cross_val_score(lin_reg, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "# lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "\n",
    "# # Linear Regression RMSE Scores\n",
    "# display_scores(lin_rmse_scores)\n",
    "\n",
    "\n",
    "# The Decision Tree Regressor seems to be a more promising model, but there are more models to try.\n",
    "\n",
    "\n",
    "# Random Forest Regressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(n_estimators=100, random_state=41)\n",
    "# forest_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# stock_predictions = forest_reg.predict(X_test)\n",
    "# forest_mse = mean_squared_error(y_test, stock_predictions)\n",
    "# forest_rmse = np.sqrt(forest_mse)\n",
    "# forest_rmse\n",
    "\n",
    "\n",
    "# forest_scores = cross_val_score(forest_reg, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "# forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "# display_scores(forest_rmse_scores)\n",
    "\n",
    "\n",
    "\n",
    "# Random forest regressor is the best until now. The score on training and validation sets are very similar and hence there should be no overfitting. If the score on the training set is much lower than that of the validation set, the data may be overfit, but in this case, it seems that the data is not overfitting (and no underfitting).\n",
    "\n",
    "\n",
    "# Now that we know the Random Forest Regressor has the lowest RMSE, lets fine-tune the parameters to get an even lower RMSE.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'bootstrap': [False], 'n_estimators': [5,10,100], 'max_features': [3,5,7]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=41)\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "grid_search.best_params_\n",
    "\n",
    "\n",
    "grid_search.best_estimator_\n",
    "\n",
    "\n",
    "final_model = grid_search.best_estimator_\n",
    "final_predictions = final_model.predict(X_test)\n",
    "# final_mse = mean_squared_error(y_test, final_predictions)\n",
    "# final_rmse = np.sqrt(final_mse)\n",
    "\n",
    "\n",
    "# print(\"Final RMSE:\",final_rmse)\n",
    "\n",
    "\n",
    "# After using the GridSearchCV to search for the best parameters and using those parameters, the final RMSE is the lowest amongst all methods tried.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelfitting import modelfitting\n",
    "\n",
    "output = modelfitting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([201000., 166700., 138000., 159500., 150340., 207000., 159740.,\n       166500., 101798., 208500.,  87700., 161645., 133500., 197500.,\n       115000., 124500., 126000., 154500., 155500., 172000.,  96016.,\n       162000., 171100., 152000., 184980., 157375., 180015., 175000.,\n       171455., 164000., 100500., 205000., 189855., 212760., 206000.,\n       205945., 214260., 163075., 148885., 192000., 203500., 197500.,\n       173995., 179095., 179500., 192000., 152765.,  95612., 161500.,\n       205805., 154585., 194845., 219420., 184500., 224000., 146000.,\n       186575., 204000.,  98000., 155250., 186500., 168500., 196295.,\n       117470., 159730., 205000., 164000., 130000., 153000., 137505.,\n       146800., 142500., 203000., 170500., 162200., 143545., 131875.,\n       120020., 203125., 147310., 140465., 145145., 158500., 150000.,\n       207500., 100648., 156000., 199000., 159810., 113875., 149500.,\n       131860., 197000., 193135., 162645., 164000., 159865., 192000.,\n       199000., 150185.,  97608., 155050., 167500., 141500., 210560.,\n       152695., 120425., 148990., 187370., 155500., 124665., 126500.,\n       127000.,  96000., 162525., 203850.,  96200., 160500., 132000.,\n        68768., 151500., 198340., 175000., 170415., 130000., 134610.,\n       202000.,  98026.,  96305., 200000., 170855., 149500., 134480.,\n       175000., 193000., 161540., 118500., 160500., 132000., 180000.,\n       198000., 134020., 162000., 152000., 193740.,  98914., 170000.,\n       185500., 157500., 166500., 150880., 151000., 151285., 127860.,\n       158000.,  89700., 118500., 160865., 202000.,  97988., 156500.,\n       192780., 203500., 173000., 169765., 164000.,  94600., 172890.,\n       159390., 136500., 162000., 157320., 180000., 135350., 125030.,\n       114000., 172660.,  98187., 162355., 107060., 195500., 179620.,\n       144500.])"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}